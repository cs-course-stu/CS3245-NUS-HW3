In this assignment, we didn't ask you to support phrasal queries, which is a feature that is typically supported in web search engines. Describe how you would support phrasal search in conjunction with the VSM model. A sketch of the algorithm is sufficient. (For those of you who like a challenge, please go ahead and implement this feature in your submission but clearly demarcate it in your code and allow this feature to be turned on or off using the command line switch "-x" (where "-x" means to turn on the extended processing of phrasal queries). We will give a small bonus to submissions that achieve this functionality correctly).

As for the index part, we use list of three parts: doc_id, tf, list of position to store the info. After stemmering the word, we use counter(term_pos in indexer.py) to count the position of each word and append it into the position list.
When we save all content into file, we first split postings list into three parts: doc_id, 1+log(tf) and list of position. Then convert each position list into numpy.array and store them into file.

As for the search part, we get the postings lists of terms first. And then, we get all the docs that all the terms in the query. These docs need to be judged again because they may contain all the terms but these terms may not adjacent. So we judging every doc if it contains the phrase. After that, these docs are treated as candidate docs for ranking. Finally, we rank these candidate docs and get the top K documents.

Describe how your search engine reacts to long documents and long queries as compared to short documents and queries. Is the normalization you use sufficient to address the problems (see Section 6.4.4 for a hint)? In your judgement, is the ltc.lnc scheme (n.b., not the ranking scheme you were asked to implement) sufficient for retrieving documents from the Reuters-21578 collection?

We use pivoted normalized document to address this problem. If a document's length is greater then the average length, we think this document should contains more terms, so we punish this document. The a document's length is too small, we will reward this document.

Take query 'stock market crash' for example. 'stock' appeared three times and 'market' appeared one time in the doc 10506, and 'stock' appeared two times in the doc 10471. Without pivoted normalized document length, 10506 get better score. However, doc 10471 is shorter than the doc 10506, so it ranks higher than the doc 10506 when the pivoted normalized document length is enabled.

Do you think zone or field parametric indices would be useful for practical search in the Reuters collection? Note: the Reuters collection does have metadata for each article but the quality of the metadata is not uniform, nor are the metadata classifications uniformly applied (some documents have it, some don't). Hint: for the next Homework #4, we will be using field metadata, so if you want to base Homework #4 on your Homework #3, you're welcomed to start support of this early (although no extra credit will be given if it's right).

I don't think it's a good idea although field parametric indices can promote the search phrase and show more accurate results. As for the Reuters dataset, there are not uniform metadata classification and some datasets even don't have metadata.
For example, data:

SYSTEMATICS INC &lt;SYST> REGULAR PAYOUT
  Qtly div three cts vs three cts prior
      Pay March 13
      Record February 27

including the information(date) of pay and record, which are the specific metadata classification some other data also have. However, getting all classification for whole data is impossible and time-consuming.
